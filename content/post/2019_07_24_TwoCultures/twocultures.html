---
title: "Breiman's two cultures"
author: "Nicholas Horton"
date: '2019-07-23'
output:
  html_document:
    df_print: paged
tags:
- predictive analytics
- regression modeling
categories: R
---



<p>Eighteen years ago, Leo Breiman published an important paper entitled <a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726">Statistical modeling: the two cultures</a> in <em>Statistical Science</em>. In today’s blog entry we discuss the implications of the paper for data science education.</p>
<p>Breiman argued that the two cultures included:</p>
<ol style="list-style-type: decimal">
<li>one that assumes a stochastic data model</li>
<li>one that uses an algorithmic model (and treats the data mechanism as unknown)</li>
</ol>
<p>Breiman asserted that the statistics community had almost exclusively focused on the former, with interpretation of parameters at the core. He asserted that:</p>
<blockquote>
<p>This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.</p>
</blockquote>
<p>When we look at the standard statistics curriculum nearly twenty years later, the lack of balance that Breiman noted still exists. Predictive analytics approaches (what Breiman calls algorithmic modeling) is rarely if ever included in standard introductory statistics courses or even in applied regression classes. (We should note that many new courses have been added to the curriculum which embrace algorithmic modeling and use excellent resources such as <a href="http://www.statlearning.com">Introduction to Statistical Learning</a>: our focus here is on the standard statistics curriculum.)</p>
<p>If you haven’t read Breiman’s paper (or haven’t done so recently) we encourage you to do so.</p>
<p>Regular readers will recall our guest blog entry on the <a href="https://teachdatascience.com/codap">CODAP</a> environment. <a href="https://iase-web.org/icots/10/proceedings/pdfs/ICOTS10_7F3.pdf">Teaching and learning about tree-based methods for exploratory data analysis</a> described how CODAP could be used to teach algorithmic models to an audience of pre-university students.</p>
<p>How hard would it be to bring in examples of algorithmic models early into statistics courses to motivate students and balance the presentation of topics?</p>
<div id="introducing-trees" class="section level3">
<h3>Introducing Trees</h3>
<p>Here’s an example where <a href="https://en.wikipedia.org/wiki/Recursive_partitioning">recursive partitioning</a> is used as an algorithmic model which predicts whether subjects in the <a href="https://cran.r-project.org/package=NHANES">NHANES</a> (National Health and Nutrition Examination Survey) dataset report any lifetime use of marijuana.</p>
<pre class="r"><code>library(rpart)
library(rpart.plot)
library(mosaic)
NHANESpot &lt;- filter(NHANES::NHANES, !is.na(Marijuana), !is.na(Work))
tally(~ Marijuana, data = NHANESpot)
## Marijuana
##   No  Yes 
## 2048 2892
tally(~ Marijuana, format = &quot;percent&quot;, data = NHANESpot)
## Marijuana
##       No      Yes 
## 41.45749 58.54251</code></pre>
<p>We see that 59% of the analytic sample report using marijuana at least once over their lifetime.</p>
<pre class="r"><code>set.seed(420)
trainrows &lt;- sample(1:nrow(NHANESpot), 4000, replace = FALSE)
NHANEStrain &lt;- NHANESpot[trainrows,]
NHANEStest &lt;- NHANESpot[-trainrows,]
tree1 &lt;- rpart::rpart(Marijuana ~ Age + Gender + Education + Work, 
               method = &quot;class&quot;, 
               control = rpart.control(minsplit = 10, cp = 0.008),
               data = NHANEStrain)
tree1
## n= 4000 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 4000 1682 Yes (0.4205000 0.5795000)  
##    2) Education=8th Grade 152   38 No (0.7500000 0.2500000) *
##    3) Education=9 - 11th Grade,High School,Some College,College Grad 3848 1568 Yes (0.4074844 0.5925156)  
##      6) Gender=female 1836  841 Yes (0.4580610 0.5419390)  
##       12) Work=NotWorking 494  233 No (0.5283401 0.4716599) *
##       13) Work=Looking,Working 1342  580 Yes (0.4321908 0.5678092) *
##      7) Gender=male 2012  727 Yes (0.3613320 0.6386680) *
rpart.plot::rpart.plot(tree1)</code></pre>
<p><img src="/post/2019_07_24_TwoCultures/twocultures_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>A relatively simple algorithmic model classifies the subjects into one of four groups: males with more than 8th grade education have a 63.9% chance of reporting use, while subjects with 8th grade had the lowest chance (25%).</p>
<p>A more complicated model can be fit: the new model could be thought of as a “black-box” model where the decisions are hard to describe (but the model might have strong predictive ability in the test data).</p>
<pre class="r"><code>tree2 &lt;- rpart::rpart(Marijuana ~ Age + Gender + Education + Work, 
               method = &quot;class&quot;, 
               control = rpart.control(minsplit = 10, cp = 0.003),
               data = NHANEStrain)
rpart.plot::rpart.plot(tree2)</code></pre>
<p><img src="/post/2019_07_24_TwoCultures/twocultures_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Recursive partitioning can be taught quickly in most levels of data science and statistics courses. We recommend the following scrollytelling visualization by Lee and Chu which breaks down the details of partitioning, <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A visual introduction to machine learning</a>.<br></p>
<center>
<figure>
<img alt = 'nycsfscrolly' width='600' src='/post/twocultures/nycsf.png' />
<figcaption>
An intuitive and visual description of how trees are built, <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A visual introduction to machine learning</a> by Stephanie Yee and Tony Chu.
</figcaption>
</figure>
</center>
<p><br></p>
<p>Breiman concluded by saying:</p>
<blockquote>
<p>If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.</p>
</blockquote>
</div>
<div id="learn-more" class="section level3">
<h3>Learn more</h3>
<ul>
<li><a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726">Statistical Modeling: The Two Cultures</a> by Leo Breiman, <em>Statistical Science</em> (2001), 16(3):199-231.</li>
<li><a href="http://www.statlearning.com">Introduction to Statistical Learning</a></li>
<li>An intuitive and visual description of how trees are built, <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">A visual introduction to machine learning</a> by Stephanie Yee and Tony Chu.</li>
</ul>
</div>
<div id="about-this-blog" class="section level3">
<h3>About this blog</h3>
<p>Each day during the summer of 2019 we intend to add a new entry to this blog on a given topic of interest to educators teaching data science and statistics courses. Each entry is intended to provide a short overview of why it is interesting and how it can be applied to teaching. We anticipate that these introductory pieces can be digested daily in 20 or 30 minute chunks that will leave you in a position to decide whether to explore more or integrate the material into your own classes. By following along for the summer, we hope that you will develop a clearer sense for the fast moving landscape of data science. Sign up for emails at <a href="https://groups.google.com/forum/#!forum/teach-data-science" class="uri">https://groups.google.com/forum/#!forum/teach-data-science</a> (you must be logged into Google to sign up).</p>
<p>We always welcome comments on entries and suggestions for new ones. However, comments on the blog should be constructive, encouraging, and supportive. We reserve the right to delete comments that violate these guidelines.</p>
</div>
