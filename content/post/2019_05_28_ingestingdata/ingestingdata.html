---
title: "Ingesting Data"
author: "Jo Hardin"
date: '2019-05-28T21:13:14-05:00'
output:
  html_document:
    df_print: paged
tags:
- R Markdown
- data wrangling
- data ingestation
categories: R
---



<div id="why-use-data-from-outside-sources" class="section level2">
<h2>Why use data from outside sources?</h2>
<p>The world is awash in data, and whatever else we teach in a data science curriculum, data must be at the center. Calls to modernize statistics and data science courses regularly point to using “real” data. The National Academies Report on Data Science for Undergraduates (see previous blog post at: <a href="https://teachdatascience.com/nasem/" class="uri">https://teachdatascience.com/nasem/</a>) reports Data Management &amp; Curation as a core part of data acumen. Indeed, they recognize data provenance to be a key skill which is “important for all students in [a] data science program.”</p>
<p>Additionally, students are typically much more engaged in the technical content if they are curious about the research question(s) posed by the data at hand. That is, if the students choose a particular dataset, they are more likely to spend time with the data practicing their data science skills. It is my experience that students are most likely to want to work with large datasets which speak to their daily lives. Capturing data from a website and making it usable in R can be a daunting task, but it is aided by R packages designed to ingest outside data into (mostly tidy) dataframes.</p>
</div>
<div id="r-packages-for-ingesting-data-from-outside-sources" class="section level2">
<h2>R packages for ingesting data from outside sources</h2>
<ul>
<li><code>googlesheets</code>: connect to googlesheets anywhere</li>
<li><code>fivethirtyeight</code>: connect to 538’s database</li>
<li><code>jsonlite</code>: parsing JSON data</li>
<li><code>xml</code>: parsing XML files</li>
<li><code>datapasta</code>: copy and paste for odd data structures in the wild</li>
<li><code>rvest</code>: package for scraping data from HTML pages</li>
<li><code>httr</code>: connect to APIs</li>
</ul>
<p>The R packages described above require different levels of computational savvy. One might start with <code>googlesheets</code> and <code>fivethirtyeight</code> as a straightforward way to import data. Then <code>jsonlite</code> and <code>xml</code> help for downloading and more sophisticated parsing of the data structure. Finally, <code>rvest</code> and <code>httr</code> allow the R user to download data which exist in alternative structures (HTML pages or APIs). Below we walk through a complete example using <code>googlesheets</code> to download data from <a href="http://www.gapminder.org/">GapMinder</a>.</p>
</div>
<div id="example-using-googlesheets-to-download-literacy-data-from-gapminder" class="section level2">
<h2>Example using <code>googlesheets</code> to download literacy data from GapMinder</h2>
<p>The example below comes from a project on creating Dynamic Datasets which was published in <a href="https://escholarship.org/uc/item/13g5g3dm"><em>Technological Innovations in Statistics Education</em></a> (Hardin, 2018). We import literacy rates measured at the country level from <a href="http://www.gapminder.org/">GapMinder</a> via <code>googlesheets</code>. Note that non-public sheets requires an additional step of authentication, see <a href="https://github.com/jennybc/googlesheets">googlesheets</a> for more information.</p>
<p>Three datasets are loaded. The datasets are the female literacy rate over time, the male literacy rate over time, and the overall literacy rate over time for dozens of countries going back to the mid-1970s. The R code is given in the Markdown file.</p>
<pre class="r"><code># the URL for the GapMinder dataset(s) of interest
litF_url &lt;- &quot;https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0&quot;
litM_url &lt;- &quot;https://docs.google.com/spreadsheets/d/1YF1_ps4srYp8GLdH38v7hJQtDDjFJWz6_5bg-_zICaY/pub?gid=0&quot;
litALL_url &lt;- &quot;https://docs.google.com/spreadsheets/d/12O0Bo85Dd-9bNq6p5KwXduPET1cRETP-mKy3ZK4q_xo/pub?gid=0&quot;


#pulling in the URL &amp; keeping track of how big it is
litFurl &lt;- googlesheets::gs_url(litF_url, visibility=&quot;public&quot;)
litF_nrow &lt;- litFurl$ws$row_extent[1]
litF_ncol &lt;- litFurl$ws$col_extent[1]

litMurl &lt;- googlesheets::gs_url(litM_url, visibility=&quot;public&quot;)
litM_nrow &lt;- litMurl$ws$row_extent[1]
litM_ncol &lt;- litMurl$ws$col_extent[1]

litALLurl &lt;- googlesheets::gs_url(litALL_url, visibility=&quot;public&quot;)
litALL_nrow &lt;- litALLurl$ws$row_extent[1]
litALL_ncol &lt;- litALLurl$ws$col_extent[1]


#reading in the datasets
litF &lt;- googlesheets::gs_read(litFurl, range=googlesheets::cell_limits(c(1,1), c(litF_nrow,litF_ncol)))
litM &lt;- googlesheets::gs_read(litMurl, range=googlesheets::cell_limits(c(1,1), c(litM_nrow,litM_ncol)))
litALL &lt;- googlesheets::gs_read(litALLurl, range=googlesheets::cell_limits(c(1,1), c(litALL_nrow,litALL_ncol)))</code></pre>
<p>Looking at the data at this point is a good idea. One thing to notice is that there is a ton of missing data. That’s expected (especially if you are used to looking at GapMinder data), because we wouldn’t expect that every country has literacy data for each gender going back 40 years for every single year. Also, notice that the data aren’t in tidy format (rows as observational units and columns of variables). After we wrangle the data below it will (a) look different and (b) be in tidy form.</p>
<pre class="r"><code>glimpse(litALL)
## Observations: 260
## Variables: 38
## $ `Adult (15+) literacy rate (%). Total` &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;,…
## $ `1975`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1976`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1977`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1978`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1979`                                 &lt;dbl&gt; 18.15768, NA, NA, NA, NA,…
## $ `1980`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1981`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1982`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1983`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1984`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, 95.40…
## $ `1985`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1986`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1987`                                 &lt;dbl&gt; NA, NA, 49.63088, NA, NA,…
## $ `1988`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1989`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1990`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1991`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1992`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1993`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1994`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1995`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1996`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1997`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1998`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `1999`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2000`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2001`                                 &lt;dbl&gt; NA, 98.71298, NA, NA, 67.…
## $ `2002`                                 &lt;dbl&gt; NA, NA, 69.87350, NA, NA,…
## $ `2003`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2004`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2005`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2006`                                 &lt;dbl&gt; NA, NA, 72.64868, NA, NA,…
## $ `2007`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2008`                                 &lt;dbl&gt; NA, 95.93864, NA, NA, NA,…
## $ `2009`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2010`                                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, N…
## $ `2011`                                 &lt;dbl&gt; 39.00000, 96.84530, NA, N…</code></pre>
<p>Each of the original googlesheets comes as a spreadsheet with country as the row and year as the column. R imports the years as column names (which are difficult to deal with as numeric column headers), and we need to <code>gather</code> the data into a format such that “Year” is one of the variable names. At the end of the wrangling process, the variables will be: country, year, litRateF, litRateM, litRateALL, and continent.</p>
<pre class="r"><code>litF &lt;- litF %&gt;% select(country=starts_with(&quot;Adult&quot;), everything()) %&gt;%
        gather(year, litRateF, -country) %&gt;%
        mutate( year = readr::parse_number(year)) %&gt;%
        filter(!is.na(litRateF)) 

litM &lt;- litM %&gt;% select(country=starts_with(&quot;Adult&quot;), everything()) %&gt;%
        gather(year, litRateM, -country) %&gt;%
        mutate( year = readr::parse_number(year)) %&gt;%
        filter(!is.na(litRateM)) 

litALL &lt;- litALL %&gt;% select(country=starts_with(&quot;Adult&quot;), everything()) %&gt;%
        gather(year, litRateALL, -country) %&gt;%
        mutate( year = readr::parse_number(year)) %&gt;%
        filter(!is.na(litRateALL))

literacy &lt;- full_join(full_join(litF, litM, by=c(&quot;country&quot;, &quot;year&quot;)), 
                     litALL, by=c(&quot;country&quot;, &quot;year&quot;))</code></pre>
<p>Now the data frame is in tidy format (rows are observational units, columns are variables), and the dataframe <code>literacy</code> has all of the information needed.</p>
<pre class="r"><code>glimpse(literacy)
## Observations: 575
## Variables: 5
## $ country    &lt;chr&gt; &quot;Burkina Faso&quot;, &quot;Central African Rep.&quot;, &quot;Kuwait&quot;, &quot;Tu…
## $ year       &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1976, 1976, 1976,…
## $ litRateF   &lt;dbl&gt; 3.182766, 8.399576, 48.015214, 45.098921, 38.124870, …
## $ litRateM   &lt;dbl&gt; 14.52849, 29.59292, 68.02863, 77.50394, 58.38611, 93.…
## $ litRateALL &lt;dbl&gt; 8.685145, 18.236172, 59.564392, 61.627683, 53.514879,…</code></pre>
<p>As mentioned before, we wouldn’t really expect every country to have literacy data for every year. However, it is straightforward to tally how many observations per country and per year exist in the dataset, if desired.</p>
</div>
<div id="modeling-the-gapminder-literacy-data" class="section level1">
<h1>Modeling the GapMinder literacy data</h1>
<p>An introductory analysis considers gender differences in literacy rates and uses a linear model on the difference between female and male literacy rates across time. In a statistics or data science class, you can show a graphical representation and discuss model assumptions including sampling and independence of residuals. The model indicates that the difference between male and female literacy rates is shrinking over time. However, we worry about the effects of other variables and encourage a more complete analysis. Indeed, there may be large biases in our model if important explanatory variables have been left out.</p>
<p>It is vital to note that the data collected here (and on all of GapMinder) is observational. Causal mechanisms cannot be implied regardless of strength of correlation, and we recommend a conversation with students about the dangers of possible confounding variables that might explain any suggested causal relationships.</p>
<pre class="r"><code>ggplot(literacy, aes(x=litRateF, y=litRateM)) + 
    geom_point(alpha=.75, aes(color=year)) + geom_abline(slope=1, intercept=0)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="/post/2019_05_28_ingestingdata/ingestingdata_files/figure-html/unnamed-chunk-6-1.png" alt="The plot shows that the higher the female literacy rate, the higher the male literacy rate.  Additionally, across the board, the male literacy rate is higher than the female literacy rate (as referenced by the y=x line)." width="672" />
<p class="caption">
Figure 1: The plot shows that the higher the female literacy rate, the higher the male literacy rate. Additionally, across the board, the male literacy rate is higher than the female literacy rate (as referenced by the y=x line).
</p>
</div>
<p>It might be interesting, however, to look at the relationship between literacy rates over time. To do that, we create a new variable which is the difference between male and female literacy rates.</p>
<pre class="r"><code>literacy &lt;- literacy %&gt;% mutate(diffLit = litRateM - litRateF)
lm(diffLit~year, data=literacy) %&gt;% tidy()
## # A tibble: 2 x 5
##   term        estimate std.error statistic     p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept)  418.      80.1         5.22 0.000000256
## 2 year          -0.204    0.0401     -5.10 0.000000470</code></pre>
<div id="learn-more" class="section level2">
<h2>Learn more</h2>
<ul>
<li><p>Vignette for using <code>googlesheets</code>: <a href="https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html" class="uri">https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html</a></p></li>
<li><p>Vignette for using <code>rvest</code>: <a href="https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html" class="uri">https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html</a></p></li>
<li><p>Mine Çetinkaya-Rundel &amp; Colin Rundel’s project describing the use of <code>rvest</code> to connect La Quinta with Denny’s: <a href="https://www.rstudio.com/resources/webinars/data-science-case-study/" class="uri">https://www.rstudio.com/resources/webinars/data-science-case-study/</a> and <a href="https://github.com/rundel/Dennys_LaQuinta_Chance" class="uri">https://github.com/rundel/Dennys_LaQuinta_Chance</a></p></li>
<li><p>R and APIs, starting with the basics: <a href="https://www.tylerclavelle.com/code/2017/randapis/" class="uri">https://www.tylerclavelle.com/code/2017/randapis/</a></p></li>
<li><p>Accessing APIs from R: <a href="https://www.r-bloggers.com/accessing-apis-from-r-and-a-little-r-programming/" class="uri">https://www.r-bloggers.com/accessing-apis-from-r-and-a-little-r-programming/</a></p></li>
<li><p>Hardin, J. <em>Dynamic Data in the Statistics Classroom</em>, <strong>TISE 11(1)</strong>, 2018: <a href="https://escholarship.org/uc/item/13g5g3dm" class="uri">https://escholarship.org/uc/item/13g5g3dm</a></p></li>
<li><p>R Markdown file for this blog post <a href="https://github.com/hglanz/teachdatascience_blog/blob/master/content/post/2019_05_28_ingestingdata/ingestingdata.Rmd">here</a></p></li>
</ul>
<div id="about-this-blog" class="section level3">
<h3>About this blog</h3>
<p>Each day during the summer of 2019 we intend to add a new entry to this blog on a given topic of interest to educators teaching data science and statistics courses. Each entry is intended to provide a short overview of why it is interesting and how it can be applied to teaching. We anticipate that these introductory pieces can be digested daily in 20 or 30 minute chunks that will leave you in a position to decide whether to explore more or integrate the material into your own classes. By following along for the summer, we hope that you will develop a clearer sense for the fast moving landscape of data science. Sign up for emails at <a href="https://groups.google.com/forum/#!forum/teach-data-science" class="uri">https://groups.google.com/forum/#!forum/teach-data-science</a> (you must be logged into Google to sign up).</p>
<p>We always welcome comments on entries and suggestions for new ones.</p>
</div>
</div>
</div>
