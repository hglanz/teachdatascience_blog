<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>parallel programming on Teach Data Science</title>
    <link>/tags/parallel-programming/</link>
    <description>Recent content in parallel programming on Teach Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>(c) 2019 Copyright Teach Data Science</copyright>
    <lastBuildDate>Thu, 11 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/parallel-programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Parallel processing and sparklyr</title>
      <link>/parallel/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/parallel/</guid>
      <description>Today’s blog entry is on parallel and grid computing. As a data science education blog, our focus is more on how to discuss ways to help students learn about high performance computing in the classroom rather than parallel computing for particular research projects (for a recent example see “Ambitious data science can be painless”). Early on in data science education it’s important to develop a foundation and precursors for future work.</description>
    </item>
    
  </channel>
</rss>